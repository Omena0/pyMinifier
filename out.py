# PyMinifier V0.0.1
# type: ignore

(
    # Boilerplate
    globals().__setitem__(
        "_s", globals().__setitem__
    ),
    # Constants
    # Code
    _s("tokenize",__import__("tokenize")),
    _s("collections",__import__("collections")),
    _s("io",__import__("io")),
    
    # Function: findCommonConstants: ['code', 'length']
    _s("findCommonConstants", lambda code,length: (_s("tokens", []),
    [((tok.type in (tokenize.STRING, tokenize.NUMBER) and (tokens.append(tok.string),
    _s("freq", Counter(tokens)),
    ([(const,cnt) for const, cnt in freq.items() if cnt > 1 and len(str(const)) > length],),
    
    ,) or (None,))[0],
    tokens.append(tok.string),
    ) for tok in tokenize.generate_tokens(io.StringIO(code).readline)][-1],
    [((tok.type in (tokenize.STRING, tokenize.NUMBER) and (tokens.append(tok.string),
    _s("freq", Counter(tokens)),
    ([(const,cnt) for const, cnt in freq.items() if cnt > 1 and len(str(const)) > length],),
    
    ,) or (None,))[0],
    tokens.append(tok.string),
    ) for tok in tokenize.generate_tokens(io.StringIO(code).readline)][-1],
    (tok.type in (tokenize.STRING, tokenize.NUMBER) and (tokens.append(tok.string),
    _s("freq", Counter(tokens)),
    ([(const,cnt) for const, cnt in freq.items() if cnt > 1 and len(str(const)) > length],),
    
    ,) or (None,))[0],
    tokens.append(tok.string),
    _s("freq", Counter(tokens)),
    ([(const,cnt) for const, cnt in freq.items() if cnt > 1 and len(str(const)) > length],),
    )[0]),
    _s("tokens", []),
    [((tok.type in (tokenize.STRING, tokenize.NUMBER) and (tokens.append(tok.string),
    _s("freq", Counter(tokens)),
    ([(const,cnt) for const, cnt in freq.items() if cnt > 1 and len(str(const)) > length],),
    
    ,) or (None,))[0],
    tokens.append(tok.string),
    ) for tok in tokenize.generate_tokens(io.StringIO(code).readline)][-1],
    [((tok.type in (tokenize.STRING, tokenize.NUMBER) and (tokens.append(tok.string),
    _s("freq", Counter(tokens)),
    ([(const,cnt) for const, cnt in freq.items() if cnt > 1 and len(str(const)) > length],),
    
    ,) or (None,))[0],
    tokens.append(tok.string),
    ) for tok in tokenize.generate_tokens(io.StringIO(code).readline)][-1],
    (tok.type in (tokenize.STRING, tokenize.NUMBER) and (tokens.append(tok.string),
    _s("freq", Counter(tokens)),
    ([(const,cnt) for const, cnt in freq.items() if cnt > 1 and len(str(const)) > length],),
    
    ,) or (None,))[0],
    tokens.append(tok.string),
    _s("freq", Counter(tokens)),
    ([(const,cnt) for const, cnt in freq.items() if cnt > 1 and len(str(const)) > length],),
    
    
)
